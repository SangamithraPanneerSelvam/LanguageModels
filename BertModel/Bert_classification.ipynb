{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "Gk3Nl6WbHVRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import libararies**"
      ],
      "metadata": {
        "id": "HZReDdyy1ES-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEQwAnW9J9tt",
        "outputId": "46ad67b5-d3e8-4156-be59-c91018f27d60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Import the bert model from transformers\"\"\"\n",
        "import transformers\n",
        "# from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "\"\"\"Import necessary libraries\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "from pylab import rcParams\n",
        "from matplotlib import rc\n",
        "import spacy\n",
        "import nltk\n",
        "import re\n",
        "# nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "I4H5VQkgiIBk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialize and load data**"
      ],
      "metadata": {
        "id": "UWzYhQpp1KiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Intialize the  constants\"\"\"\n",
        "RANDOM_SEED=42\n",
        "class_names=42 #since there are 41 creators in total\n",
        "BATCH_SIZE=16\n",
        "MAX_LEN=384\n",
        "EPOCHS=5\n",
        "LEARNING_RATE=2e-5\n",
        "PRE_TRAINED_MODEL_NAME='distilbert-base-uncased'\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yjssjjDibC1",
        "outputId": "cd783ce6-7445-4ae5-90ee-3b980d467df4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"superheroes_nlp_dataset.csv\",encoding=\"utf-8\")\n",
        "dataset=data[['history_text','creator']]"
      ],
      "metadata": {
        "id": "OvTlzvHTid8s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the pre-trained model**"
      ],
      "metadata": {
        "id": "Tv2FO63a1Vuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load the pretrained model from transformers and tweak it with our extracted features \"\"\"\n",
        "class DistillBERTClass(torch.nn.Module):\n",
        "\n",
        "   def __init__(self,n_classes,PRE_TRAINED_MODEL_NAME):\n",
        "     super(DistillBERTClass,self).__init__()\n",
        "     self.bert=transformers.DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "     self.drop=nn.Dropout(p=0.2)\n",
        "     self.drop = torch.nn.Dropout(0.3)\n",
        "     self.out = torch.nn.Linear(self.bert.config.hidden_size,n_classes)\n",
        "    \n",
        "   def forward(self,input_ids, attention_mask):\n",
        "        distilbert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        output_1 = self.drop(pooled_output)\n",
        "        output = self.out(output_1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "01PcOhciiA3G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load the bert model, tokenizer from transformers\"\"\"\n",
        "bert_model=DistilBertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "model=DistillBERTClass(class_names,PRE_TRAINED_MODEL_NAME)\n",
        "model.to(device)\n",
        "tokenizer=DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHuBEROrii0w",
        "outputId": "a4238aa8-0baf-47b5-d80b-ee93e08ed34c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess the data**"
      ],
      "metadata": {
        "id": "-gGTJmWQ1f1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m3mp53Q4-80P"
      },
      "outputs": [],
      "source": [
        "\"\"\"Clean the data\"\"\"\n",
        "def preprocess(text,stop_words,stemming=False,lemantizing=False):\n",
        "  \n",
        "  text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "  #tokenize\n",
        "  text_list=text.split()\n",
        "  # text_list=[str(word) for word in text_list]\n",
        "  #stopwords\n",
        "  if stop_words:\n",
        "    text_list=[word for word in text_list if word not in stop_words]\n",
        "  \n",
        "  ## Stemming \n",
        "  if stemming ==True:\n",
        "    st=nltk.stem.porter.PorterStemmer()\n",
        "    text_list=[st.stem(word) for word in text_list]\n",
        "    \n",
        "  ## Lemmatisation \n",
        "  if lemantizing==True:\n",
        "    le=nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    text_list=[le.lemmatize(word) for word in text_list]\n",
        "\n",
        "   ## tokenize to string again\n",
        "    text = \" \".join(text_list)\n",
        "    return text\n",
        " \n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Clean the data\"\"\"\n",
        "stopwords_list=nltk.corpus.stopwords.words(\"english\")\n",
        "dataset[\"clean_history\"]=dataset[\"history_text\"].apply(lambda x: preprocess(x,stop_words=stopwords_list,stemming=False,lemantizing=True))\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "dataset[\"creator_labels\"] = labelencoder.fit_transform(dataset[\"creator\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eou7Sl6LjEZe",
        "outputId": "9bb3064d-d0f2-4c99-845a-27abcbf1a20b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split and load the data**"
      ],
      "metadata": {
        "id": "idI-vsdu1pFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Split the data\"\"\"\n",
        "# X_train,X_test,Y_train,Y_test=train_test_split(self.dataset[\"clean_history\"],self.dataset[\"creator_labels\"],test_size=0.2,random_state=self.RANDOM_SEED)\n",
        "df_train,df_test=train_test_split(dataset,test_size=0.2,random_state=RANDOM_SEED)\n",
        "    "
      ],
      "metadata": {
        "id": "XEv76ppPjPNs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Encode class to encode the text data into tokens and extract input ids, attention masks that arec necessary for the Bert model\"\"\"\n",
        "class Encode(Dataset):\n",
        "\n",
        "  def __init__(self,sentences, targets,tokenizer,max_len):\n",
        "    self.sentences=sentences\n",
        "    self.targets=targets\n",
        "    self.tokenizer=tokenizer\n",
        "    self.max_len=max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sentences)\n",
        "\n",
        "  def __getitem__(self,item):\n",
        "    Hero_story=self.sentences[item]\n",
        "    creator_name=self.targets[item]\n",
        "    encoding=self.tokenizer.encode_plus(Hero_story,max_length=self.max_len,\n",
        "                                   pad_to_max_length=True,\n",
        "                                   add_special_tokens=True,\n",
        "                                   return_token_type_ids=False,\n",
        "                                   return_tensors='pt',\n",
        "                                   return_attention_mask=True,\n",
        "                                   truncation=True)\n",
        "    \n",
        "    return{\n",
        "        'Hero_History': Hero_story,\n",
        "        'input_ids':encoding['input_ids'].flatten(),\n",
        "        'attention_mask':encoding['attention_mask'].flatten(),\n",
        "        'targets':torch.tensor(creator_name,dtype=torch.long)\n",
        "\n",
        "    }"
      ],
      "metadata": {
        "id": "X2Zm8oYNiEB4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load the data using data loader\"\"\"\n",
        "def data_loader(df):\n",
        "   \n",
        "  loader=Encode(sentences=df.clean_history.to_numpy(),targets=df.creator_labels.to_numpy(),tokenizer=tokenizer,max_len=MAX_LEN)\n",
        "  \n",
        "  return DataLoader(loader,num_workers=2,batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "mU72NGatjVUv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"load the data using data loader and tune the hyperparameters\"\"\"\n",
        "train_loader=data_loader(df_train)\n",
        "# test_loader=self.data_loader(df_test)\n",
        "\n",
        "optimizer=AdamW(model.parameters(),lr=LEARNING_RATE,correct_bias=False)\n",
        "total_steps=len(train_loader) *EPOCHS\n",
        "scheduler=get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
        "loss_fn=nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wejYau2sjbTG",
        "outputId": "eddd5257-020b-498d-e4ff-75cb9425ae4a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the model**"
      ],
      "metadata": {
        "id": "2mX39XtR2E6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model,\n",
        "        train_loader,    \n",
        "        loss_fn, \n",
        "        optimizer, \n",
        "        scheduler, \n",
        "        n_total):    \n",
        "    \n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    \n",
        "    for d in train_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double()/n_total, np.mean(losses)\n",
        "\n",
        "   \n",
        "  "
      ],
      "metadata": {
        "id": "9VUjXE3ai_ku"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f\"Epoch {epoch +1}/{EPOCHS}\")\n",
        "  print('-'*10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f\"Trainloss {train_loss} accuracy {train_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMIQWB7XjpSQ",
        "outputId": "2027fd3d-8019-42f7-b758-e419c3ad1944"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainloss 1.9384416799022728 accuracy 0.40086206896551724\n",
            "Epoch 2/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainloss 1.347890503602485 accuracy 0.6482758620689655\n",
            "Epoch 3/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainloss 0.9936418484335077 accuracy 0.7517241379310344\n",
            "Epoch 4/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainloss 0.8150594046671097 accuracy 0.7862068965517242\n",
            "Epoch 5/5\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainloss 0.7184853677267897 accuracy 0.8137931034482758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Predict**"
      ],
      "metadata": {
        "id": "0wXygsmX2Jnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self,model,new_text):\n",
        "\n",
        "\n",
        "    encoding=self.tokenizer.encode_plus(new_text,max_length=384,\n",
        "                                    pad_to_max_length=True,\n",
        "                                    add_special_tokens=True,\n",
        "                                    return_token_type_ids=False,\n",
        "                                    return_tensors='pt',\n",
        "                                    return_attention_mask=True,\n",
        "                                    truncation=True)\n",
        "      \n",
        "      \n",
        "      \n",
        "    input_ids=encoding['input_ids'].to(self.device)\n",
        "    attention_mask=encoding['attention_mask'].to(self.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # text = torch.tensor(encoding)\n",
        "        outputs=model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _,preds=torch.max(outputs,dim=1)\n",
        "        print(preds)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1NV3sYmZjtFA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}